{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkEnv_Learning_01\n",
    "\n",
    "빅데이터 - 스칼라(scala), 스파크(spark)로 시작하기: <https://wikidocs.net/28387>     \n",
    "\\[SPARK\\]Tutorial(pyspark) : <https://yujuwon.tistory.com/entry/spark-tutorial>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark    # pyspark\n",
    "import findspark  # findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# findspark\n",
    "\n",
    "- findspark 팩키지를 통해서 스파크를 찾아내고 pyspark.SparkContext 명령어로 스파크 접속지점을 특정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "- RDD는 외부데이터를 읽어서 처리하거나 자체적으로 컬렉션 데이터를 생성하여 처리할 수 있다. \n",
    "- 데이터 처리는 파티션 단위로 분리해서 작업을 처리한다. \n",
    "\n",
    "\n",
    "- RDD 타입 \n",
    "    - 트랜스포메이션(transformation)\n",
    "        - 필터링 같은 작업으로 RDD에서 새로운 RDD를 반환 \n",
    "    - 액션(action) \n",
    "        - RDD로 작업을 처리하여 결과를 반환\n",
    "        - 실행될 때마다 새로운 연산을 처리 \n",
    "            - 만약 작업의 처리 결과를 재사용하고 싶으면 persist() 메소드를 사용하여 결과를 메모리에 유지할 수 있다. \n",
    "            \n",
    "            \n",
    "- RDD는 SparkContext객체를 이용하여 생성이 가능하다. \n",
    "    - SparkContext\n",
    "        - SparkConf 객체를 이용해서 파라메터값을 설정 혹은 생성한다. \n",
    "        - 초기화도 가능하다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 데이터 이용\n",
    "\n",
    "1. 내부 데이터를 이용하는 방법(Parallelized Collections)\n",
    "    - parallelize() 메소드를 이용\n",
    "        - 연산 : map(), reduce(), filter() 등의 RDD 연산을 이용해서 처리한다. \n",
    "\n",
    "\n",
    "2. 외부데이터 이용 \n",
    "    - textFile() 메소드를 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Bigdata\\\\spark-2.4.5-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark 세션을 생성해주기위해서 다음과 같이 컴파일을 진행해준다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local[2]')\n",
    "# sc = SparkContext(master='local[2]', appName='appName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만약 세션이 끝난다면 다음과 같이 코드를 실행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD만들기\n",
    "\n",
    "RDD를 생성하기 위해서는 sc.parallelize()를 사용해야한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트에서 RDD 생성 \n",
    "data = list(range(1,6))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputdata의 새로운 집합을 생성하기위함 \n",
    "rdd = sc.parallelize(data, 4) # data를 메모리에 저장될때 4조각으로 쪼개서 메모리에 저장 \n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd.map(lambda x: x * 2)\n",
    "# map() : 데이터를 가공한다. 반환타입이 같지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect()는 액션이며 실제로 collect()가 호출되면 RDD가 메모리에 올려져 계산이 이루어진다. \n",
    "# 각 테스크의 엔트리들을 수집한후 그결과를 다시 SparkContext전송한다.\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.filter(lambda x: x % 2 == 0)\n",
    "# filter() : 함수의 결과가 참인경우에만 요소들을 통과시키는 함수이다. 결과로 새로운 RDD를 생성한다 \n",
    "# 액션은 아니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start tset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter를 테스트 해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ten(val):\n",
    "    if(val<10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "Filter_test = rdd1.filter(ten)\n",
    "Filter_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 조건에 해당하는 데이터만 선별해 오는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End test\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 3]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([1, 3, 2, 3, 4])\n",
    "rdd3.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 1, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.parallelize([1, 4, 2, 2, 3])\n",
    "rdd3.distinct().collect()\n",
    "# distinct() : 중복을 제거한 RDD를 반환한다. \n",
    "\n",
    "# 의문 : 왜 순서가 바뀌었을까? - 포기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6], [2, 7], [3, 8]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = sc.parallelize([1, 2, 3])\n",
    "rdd4.map(lambda x: [x, x+5]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 2, 7, 3, 8]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.flatMap(lambda x: [x, x+5]).collect()\n",
    "# 차원 변경 ?\n",
    "# iterator 안에 포함된 값으로 RDD를 구성하기 원할 경우에 flatmap()을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "- reduce(func)  \n",
    "- take(n)  \n",
    "- collect()  \n",
    "- takeOrdered(n, key=func) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.reduce(lambda a, b : a * b)\n",
    "\n",
    "# reduce(func): 계산된 값을 하나로 합쳐준다. \n",
    "# reduce은 파티션 레벨 단위로 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)\n",
    "# take(): RDD에서 해당 개수만큼 데이터를 가져온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = sc.parallelize([5, 3, 1, 2])\n",
    "rdd5.takeOrdered(3, lambda s: -1 * s)\n",
    "# takeOrdered() : 해당 개수만큼 데이터를 가져오는데 정렬해서 가져온다.(오름차순, 내림차순)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[27] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start tset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 세트를 넘겨주고 RDD를 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of plc_RDD: <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 생성\n",
    "# tmp_data = list(range(1,10001))\n",
    "tmp_data = range(1,10001)\n",
    "\n",
    "# 담자 \n",
    "plc_RDD = sc.parallelize(data,10) # 파티션은 제한이 없는것인가? \n",
    "print('type of plc_RDD: {0}'.format(type(plc_RDD)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해당 RDD의 파티션 숫자를 확인\n",
    "plc_RDD.getNumPartitions()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(10) ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:195 []'\n"
     ]
    }
   ],
   "source": [
    "print(plc_RDD.toDebugString())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plc_RDD id: 2\n"
     ]
    }
   ],
   "source": [
    "print('plc_RDD id: {0}'.format(plc_RDD.id())) # RDD id 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End test\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
